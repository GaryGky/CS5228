{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "787356d6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### CS5228 Assignment 3\n",
    "\n",
    "Hello everyone, this assignment notebook covers Graph Mining. There are some code-completion tasks and question-answering tasks in this answer sheet. For code completion tasks, please write down your answer (i.e., your lines of code) between sentences that \"Your code starts here\" and \"Your code ends here\". The space between these two lines does not reflect the required or expected lines of code. For answers in plain text, you can refer to [this Markdown guide](https://medium.com/analytics-vidhya/the-ultimate-markdown-guide-for-jupyter-notebook-d5e5abf728fd) to customize the layout (although it shouldn't be needed).\n",
    "\n",
    "When you work on this notebook, you can insert additional code cells (e.g., for testing) or markdown cells (e.g., to keep track of your thoughts). However, before the submission, please remove all those additional cells again. Thanks!\n",
    "\n",
    "**Important:** \n",
    "* Remember to rename and save this Jupyter notebook as **A3_YourName_YourNUSNETID.ipynb** (e.g., **A3_BobSmith_e12345678.ipynb**) before submission! Failure to do so will yield a penalty of 1 Point.\n",
    "* Remember to rename and save the script file *A3_script.py* as **A3_YourName_YourNUSNETID.py** (e.g., **A3_BobSmith_e12345678.py**) before submission! Failure to do so will yield a penalty of 1 Point.\n",
    "* Submission deadline is Oct 30, 11.59 pm. Late submissions will be penalized by 10% for each additional day.\n",
    "\n",
    "Please also add your nusnet and student id in the code cell below. This is just to make any identification of your notebook doubly sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45155a8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "student_id = ''\n",
    "nusnet_id = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4237ac",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here is an overview over the tasks to be solved and the points associated with each task. The notebook can appear very long and verbose, but note that a lot of parts provide additional explanations, documentation, or some discussion. The code and markdown cells you are supposed to complete are well, but you can use the overview below to double-check that you covered everything.\n",
    "\n",
    "* **1 Recommender Systems (20 Points)**\n",
    "    * 1.1 Matrix Factorization (15 Points)\n",
    "        * 1.1 a) Implement method `fit()` (8 Points)\n",
    "        * 1.1 b) Explore different hyperparameter settings (2 Points)        \n",
    "        * 1.1 c) Matrix Factorization & Updates (5 Points)\n",
    "    * 1.2 Questions about Recommender Systems (5 Points)\n",
    "* **2 Graph Mining (27 Points)**\n",
    "    * 2.1 Centrality (20 Points)\n",
    "        * 2.1 a) Implement Closeness Centrality (4 Points)\n",
    "        * 2.1 b) Implement PageRank Centrality (7 Points\n",
    "        * 2.1 c) Comparing Centrality Measures (5 Points)\n",
    "        * 2.1 d) Discussion of Limitation of Results (4 Points)\n",
    "    * 2.2 Community Detection (10 Points)\n",
    "        * 2.2 a) Implement Girvan-Newman Algorithm (4 Points)\n",
    "        * 2.2 b) Question about Girvan-Newman Algorithm (6 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ebfdc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setting up the Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5ed701",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Using this mode of visualization will allow you zoom in into plot, which will be convenient for some tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24134c99",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edd4e2c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3786b25",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from src.utils import plot_mrt_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464c5811",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "**Important:** In this notebook, most code-completion tasks require to edit the file `A3_script.py`. The code cell below ensures that any change to the file (after saving) will cause a reload in this notebook. So there's no need to \"manually\" import the code after every change. Way more convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328b0bde",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from A3_script_SOLUTION import closeness, pagerank, girvan_newman, NMF\n",
    "#from A3_BobSmith_e12345678 import closeness, pagerank, girvan_newman, NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde8ac7a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1 Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f31c9c9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.1 Matrix Factorization (15 Points)\n",
    "\n",
    "Matrix Factorization -- and here more specifically: non-negative Matrix Factorization -- is a class of algorithms where a matrix $M$ is factorized into (usually) two matrices $W$ and $H$, with the property that all three matrices have no negative elements. Matrix Factorization is popular techniques applied in recommender systems, where $W$ and $H$ contain a latent representation of all users and all items, respectively, and $M$ represents the rating matrix.\n",
    "\n",
    "In this task, you will implement (non-negative) Matrix Factorization from scratch using Gradient Descent as covered in the lecture. In fact, we use the rating matrix $M$ which was used as an example in the lecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7895711",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "M = np.array([\n",
    "    [4, 0, 0, 5, 1, 0, 0],\n",
    "    [5, 5, 4, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 2, 4, 5, 0],\n",
    "    [0, 3, 0, 0, 0, 0, 3]\n",
    "], dtype=np.float)\n",
    "\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b905a8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We provide you with the skeleton code for class `NMF` (short for Non-Negative Matrix Factorization). The code includes the initialization of matrices `W` and `H`, as well as of Matrix `Z`. Matrix `Z` is an auxiliary matrix containing the indices of all non-zero entries of Matrix `M`. Recall from the lecture that we need to compute the Gradient Descent only based on the non-zero entries in the rating matrix.\n",
    "\n",
    "The code cell below shows an example using the default parameter (`k=100`). The shapes of `W` and `H` reflect the number of users and items, as well as the size $k$ of the latent representations. The shape of `Z` is `(num_nonzero, 2)`. For example matrix `M`, the shape should be `(11, 2)` since `M` has 11 non-zero entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebe5113",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "nmf = NMF(M)\n",
    "\n",
    "print('W.shape = {}'.format(nmf.W.shape))\n",
    "print('H.shape = {}'.format(nmf.H.shape))\n",
    "print('Z.shape = {}'.format(nmf.Z.shape))\n",
    "print()\n",
    "print('Z containing all the indices of all non-zero entries in M (first 5 entries only)')\n",
    "print(nmf.Z[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66282dac",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We also provide you with the method `calc_loss()` which calculates the loss w.r.t. the current values of matrices `W` and `H`. **Important:** Note that method implements the loss without regularization! Since we need this method only to print the loss and so to see its trend over time, this simplified calculation is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ece6924",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "nmf = NMF(M)\n",
    "\n",
    "loss = nmf.calc_loss()\n",
    "\n",
    "print('Initial loss: {:.1f}'.format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f465e317",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You should see an initial loss of **4879.6**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9199da62",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**1.1 a) Implement method `fit()` to perform matrix factorization using Gradient Descent! (8 Points).** The complete algorithm together with the required gradients is available as pseudo code in the lecture slides, and you are already familiar with the basic concept of Gradient Descent. Here, consider the regularization terms when calculating the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec972a9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "nmf = NMF(M)\n",
    "\n",
    "nmf.fit(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d1d66b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With the default values for all parameters  (`k=100`, `learning_rate=0.0001`, `lambda_reg=0.1`, `num_iter=100`), you should see a loss around **167.6** at the end of the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cb2ad7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Predicting unknown ratings (nothing for you to do here).** With our learned estimates for `W` and `H`, we can simply calculate matrix `P` as the product of `W` and `H`, representing the matrix of predicted ratings. We encapsulate this simple computation in method `predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d122b46",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "P = nmf.predict()\n",
    "\n",
    "print(np.around(P, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6913b7d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With the default values for all parameters  (`k=100`, `learning_rate=0.0001`, `lambda_reg=0.1`, `num_iter=100`), the result should look something like this:\n",
    "\n",
    "```\n",
    "[[ 7.02 10.17 11.97  7.85  5.61 10.61 12.52]\n",
    " [ 7.75  6.9   8.05 11.22  9.09 14.9  13.09]\n",
    " [ 9.65  8.96 10.37  7.02  6.81  8.33 10.76]\n",
    " [ 9.11  7.25 10.69 11.67  9.07 12.4   9.27]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0523a7b4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**1.2 d) Explore different hyperparameter settings and briefly explain your observations! (2 Points)** You can use the code cell below for that; you can simply set different values for `k`, `learning_rate`, `lambda_reg`, and `num_iter`. Note that it's not about find the *best* values for those parameters but to observe how changing those values affect the result.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2880112",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b0f7ae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "k, learning_rate, lambda_reg, num_iter = 100, 0.0001, 0.1, 100\n",
    "\n",
    "nmf = NMF(M, k=k)\n",
    "\n",
    "nmf.fit(learning_rate=learning_rate, lambda_reg=lambda_reg, num_iter=num_iter, verbose=True)\n",
    "\n",
    "P = nmf.predict()\n",
    "\n",
    "print('\\nReconstructed rating matrix:')\n",
    "print(np.around(P, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849d1d0b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.1 e) Matrix Factorization & Updates (5 Points)\n",
    "\n",
    "You have now implemented a basic model-based recommender system using (non-negative) Matrix Factorization. Since we used only a toy rating matrix, performance was not an issue here. In real-world recommendations with many users and items, Matrix Factorization can be quite time consuming. The problem is that online platforms are very dynamic: users are joining and leaving, new items are added, users add new or update previous ratings. All of those cases change the rating matrix.\n",
    "\n",
    "**How do different cases (e.g., new user/item/rating) affect a current result of a Matrix Factorization for a recommender system? (3 Points)** Outline the different problems, and discuss meaningful approaches to mitigate them. For example, a new user or item refers to the *Cold-Start Problem*. What are good practical strategies to address the Cold-Start Problem and other changes to the rating matrix using Matrix Factorization?\n",
    "\n",
    "(Note: When you're discussing challenges regarding runtime/performance, please **exclude** any solutions relying on bigger clusters and parallel computing :). While those are valid points, in principle, here we want to focus on conceptual solutions).\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5f5fb4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00acd828",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2 Questions about Recommender Systems (5 Points)\n",
    "\n",
    "**True/False Questions about Recommender Systems**: In the table below are 5 statements that are either True or False. Complete the table to specify whether a statement is True or False, and provide a brief explanation for your answer (Your explanation is more important than a simple True/False answer)\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579911df",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is a markdown cell. Please fill in your answers for (1)~(5).\n",
    "\n",
    "| No. | Statement                                                                                                   | True or False?       | Brief Explanation |\n",
    "|-----|------------------------------------------------------------------------------------------------------------|--------------| ------- |\n",
    "| (1)  | You have a running recommender system using 1-5 star ratings. Now you change it to a 1-10 scoring system. This makes your 1-5 star ratings obsolete. | True/False |     |\n",
    "| (2)  | Recommendation engines can limit users' exposure to a wider variety of items. | True/False |    |\n",
    "| (3)  | Most recommendation engines benefit from some degree of randomization when providing recommendations to a user. | True/False |     |\n",
    "| (4)  | A CF-based recommender system will always outperform a Content-Based Recommender System | True/False|  |\n",
    "| (5)  | For User-Based CF, we normalize the ratings to ensure that all ratings of a user sum up to 1. |True/False |   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8cf7e2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d411a8f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2 Graph Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91c8f55",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load and Prepare Data\n",
    "\n",
    "Throughout this section we work the MRT train network as our underlying graph. The MRT stations mark the nodes, and there is an edge (directed or undirected; see below) if there is a direct train connection between the respective MRT stations.\n",
    "\n",
    "**Load data from files.** We first load the information about the MRT stations. We only need this information to have access to the latitude and longitude of the stations, so we can plot the MRT graph and preserve the relative geographic locations of the MRT stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0092ac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_mrt_stations = pd.read_csv('data/a3-mrt-stations.csv')\n",
    "\n",
    "df_mrt_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d071a751",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following file contains the main information: Which MRT stations are directly connected with by a train. Not that the file contains each connection twice for both directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38fc4a0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_mrt = pd.read_csv('data/a3-mrt-connections.csv')\n",
    "\n",
    "df_mrt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef0e3de",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create Graphs\n",
    "\n",
    "From this data, we can easily create 2 NetworkX graphs. We create an undirected graph `G_undirected` and a direct graph `G_directed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cc89c3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Create an \"empty\" undirected and directed graph\n",
    "G_undirected = nx.Graph()\n",
    "G_directed = nx.DiGraph()\n",
    "\n",
    "for idx, row in df_mrt.iterrows():\n",
    "    G_undirected.add_edge(row['to'], row['from'])\n",
    "    G_directed.add_edge(row['to'], row['from'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6653a545",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We provide you with the method `plot_mrt_graph()` to visualize the train network. As mentioned before, we can utilize the information about the geocoordinates of MRT station to preserve their relative location. Of course mthe connections between the nodes / MRT stations are still just straight lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8262c311",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_mrt_graph(G_undirected, df_mrt_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc2988b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.1 Centrality (12 Points)\n",
    "\n",
    "We first explore the concept of Centrality which aims to identify \"important\" nodes in a Graph. As we saw in the lecture, there is a wider variety of Centrality measures to look at different aspects of a node and the whole graph to compute a node's importance. In the following, we look at *Closeness* and *PageRank*.\n",
    "\n",
    "#### 2.1 a) Implement Closeness Centrality (4 Points)\n",
    "\n",
    "The Closeness Centrality of a node $v$ is defined as\n",
    "\n",
    "$$\n",
    "closeness(v) = \\frac{N}{\\sum_{w\\in V}d(v,w)}\n",
    "$$\n",
    "\n",
    "where $N$ is the number of nodes that can be reached from $v$, and $d(v,w)$ is the length of the shortest path between node $v$ and a node $w$.\n",
    "\n",
    "We saw that both distance-based centrality measure Closeness and Betweenness require the to solve the All-Pairs Shortest Paths (APSP) problem. Since this is not a \"programming\" or \"algorithms and data structures\" module, we don't expect you to come up with your own solution for the problem from scratch. For this task, you can utilize any method from [`nx.algorithms.shortest_paths`](https://networkx.org/documentation/stable/reference/algorithms/shortest_paths.html). Using a method to compute the shortest path between two nodes will make the computation of Closeness Centrality pretty straightforward.\n",
    "\n",
    "**Implement method `closeness()` to compute the Closeness Centrality of a Graph G.** You can assume the input Graph G being strongly connected, undirected, and unweighted.\n",
    "\n",
    "You can use the code cell below to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a1e700",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "my_closeness_scores = closeness(G_undirected)\n",
    "\n",
    "for station, score in sorted(my_closeness_scores.items(), key=lambda kv: kv[1], reverse=True)[:5]:\n",
    "    print('{} ({:.5f})'.format(station, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89de5c2f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Compare your implementation with the one from NetworkX**. The code cell belows computes the Closeness Centrality over the *undirected* MRT graph using the implementation from NetworkX, and again shows the 5 MRT stations with the highest scores. Apart from minor precision issues, the NetworkX result and your result should match, of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac24661",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nx_closeness_scores = nx.algorithms.centrality.closeness_centrality(G_undirected)\n",
    "\n",
    "for station, score in sorted(nx_closeness_scores.items(), key=lambda kv: kv[1], reverse=True)[:5]:\n",
    "    print('{} ({:.5f})'.format(station, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0392ff0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.1 a) Implement PageRank Centrality (8 Points)\n",
    "\n",
    "In this task, you will implement the basic PageRank algorithm using the Power Iteration methods as introduced in the lecture.\n",
    "\n",
    "$$\n",
    "c_{PR} = \\alpha M c_{PR} + (1-\\alpha)E\n",
    "$$\n",
    "\n",
    "where $E = (1/n, 1/n, ..., 1/n)^T$ with $n$ being the number of nodes.\n",
    "\n",
    "Recall from the lecture that PageRank requires the **transition matrix** of a graph is input. For this, we provide you with the method `create_transition_matrix(A)` that converts the adjacency matrix of a Graph G into an transition matrix. Check out also the given code in method `pagerank()` where we use a numpy method to convert the Graph G to its adjacency matrix and then call `create_transition_matrix(A)`.\n",
    "\n",
    "**Implement method `pagerank()` to compute the PageRank Centrality of a Graph G**.  You can assume the input Graph G being strongly connected, directed, and unweighted.\n",
    "\n",
    "You can use the code cell below to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9be09b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "my_pagerank_scores = pagerank(G_directed)\n",
    "\n",
    "for station, score in sorted(my_pagerank_scores.items(), key=lambda kv: kv[1], reverse=True)[:5]:\n",
    "    print('{} ({:.5f})'.format(station, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0838ab3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Compare your implementation with the one from NetworkX**. The code cell belows computes the PageRank Centrality over the *directed* MRT graph using the implementation from NetworkX, and again shows the 5 MRT stations with the highest scores. Apart from minor precision issues, the NetworkX result and your result should match, of course. Note that your implementation and the one of NetworkX are using the same default value for `alpha` and `eps` (called `tol` in case of NetworkX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fcb5c9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nx_pagerank_scores = nx.pagerank(G_directed)\n",
    "\n",
    "for station, score in sorted(nx_pagerank_scores.items(), key=lambda kv: kv[1], reverse=True)[:5]:\n",
    "    print('{} ({:.5f})'.format(station, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a92cb3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.1 c) Comparing Centrality Measures (5 Points)\n",
    "\n",
    "In the lecture, we covered several more Centrality measures. The table below shows the top-5 MRT stations with respect to their scores w.r.t to most of the measures we talked about. Note that we don't care about the exact scores, but just about the ranking these scores induce.\n",
    "\n",
    "\n",
    "| Rank | PageRank | InDegree | OutDegree | Closeness | Betweenness |\n",
    "| ---  | ---      | ---      | ---       | ---         |  --- |\n",
    "| 1    |  woodlands | dhoby ghaut  | dhoby ghaut |little india |  botanic gardens  |\n",
    "| 2    |  dhoby ghaut  | macpherson | macpherson | botanic gardens | buona vista |\n",
    "| 3    | tampines  | little india  | little india | newton  | bishan  |\n",
    "| 4    | buona vista  | buona vista | buona vista | caldecott | serangoon |\n",
    "| 5    | serangoon | chinatown| chinatown | stevens | caldecott |\n",
    "\n",
    "**Discuss the rankings and any interesting observations.** Based on the definitions and intuitions behind these 5 different centrality measures, discuss the rankings from the table above: For each centrality measure, briefly describe in your own words what it means for a MRT station to have the highest score!\n",
    "\n",
    "(Note: Difference between the measures w.r.t. complexity and performance are not relevant here)\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5b1e72",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fcd6718",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.1 d) Discussion of Limitation of Results (4 Points)\n",
    "\n",
    "The table in 2.1 c) shows us the top-5 MRT stations with respect to different centrality measures. While this is interesting in itself, we usually want this information to make any informed decision depending on a given application scenario. For example, the Government, the Land Transport Authority (LTA), Urban Redevelopment Authority (URA), or private home buyers have very different information needs from the MRT graph.\n",
    "\n",
    "Come up with **TWO (2)** concrete application scenarios utilizing the MRT graph. For each scenario\n",
    "\n",
    "* briefly motivate which centrality measure is arguably the most suitable, and\n",
    "\n",
    "* briefly discuss the practical limitation that might yield subpar results\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae6ba60",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ede1fc89",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 Community Detection (10 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fbf87c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.2 a) Implement Girvan-Newman Algorithm (4 Points)\n",
    "\n",
    "The Girvan-Newman Algorithm finds communities in a graph by assuming a strongly connected graph and then iteratively removing a minimum set of edges until the graph breaks into 2 components. The criteria to remove an edge is based on the Edge Betweenness Centrality; cf. lecture slides. The Edge Betweenness Centrality $c_{B}(e)$ of an Edge $e$ given a Graph $G=(V,E)$ is defined as:\n",
    "\n",
    "$$c_{B}(e) = \\sum_{u,w\\in V} \\frac{\\sigma(v,w|e)}{\\sigma(v,w)}$$\n",
    "\n",
    "where $\\sigma(v,w)$ is the number of shortest paths from $v$ to $w$, and $\\sigma(v,w|e)$ is the number of shortest paths from $v$ to $w$ going through Edge $e$.\n",
    "\n",
    "Similar to the more traditional Betweenness Centrality for nodes, Edge Betweenness Centrality also fundamentally requires solving the All-Pairs Shortest Paths (APSP) problem. As such, we could again utilize [`nx.algorithms.shortest_paths`](https://networkx.org/documentation/stable/reference/algorithms/shortest_paths.html). However, it gets a bit tedious since we here need **all** shortest paths between **all** pairs of nodes. So you can simply use [`nx.algorithms.centrality.edge_betweenness_centrality`](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.edge_betweenness_centrality.html) here.\n",
    "\n",
    "**Implement the method `girvan_newman()` split a Graph G into 2 components!** You can assume that the Graph is undirected, unweighted, and strongly connected. Together with being able to use So you can simply use [`nx.algorithms.centrality.edge_betweenness_centrality`](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.edge_betweenness_centrality.html), this should make it a rather straightforward task. Here are 2 additional constraints:\n",
    "\n",
    "* If 2 or more edges have the same maximum edge centrality, still remove only **1 edge** per iteration (you can randomly pick one of the edges with maximum edge centrality)\n",
    "* Include a print statement that shows which edge has been removed in an iteration (see given code of method `girvan_newman()`); this is merely so you can check your implementation with the expected outcome below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cddfa0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "communities, G_split = girvan_newman(G_undirected, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac8bebe",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The expect output for the code cell above is:\n",
    "\n",
    "```\n",
    "Edge ('farrer road', 'botanic gardens') removed (edge betweenness centrality: 0.211)\n",
    "Edge ('outram park', 'tiong bahru') removed (edge betweenness centrality: 0.217)\n",
    "Edge ('harbourfront', 'outram park') removed (edge betweenness centrality: 0.245)\n",
    "Edge ('marsiling', 'woodlands') removed (edge betweenness centrality: 0.380)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba80d489",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The return Graph `G_split` is the original graph without the edges that needed to be removed to break of the original Graph `G` in to 2 components. As such, we can now plot `G_split` to visualize the 2 components. You need to zoom in to the MRT connections that reflect the removed edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc92d9d4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_mrt_graph(G_split, df_mrt_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337fcbd6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.2 b) Question to Girvan-Newman Algorithm (6 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90826378",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is a markdown cell. Please fill in your answers for (1)~(3), 2 Points each.\n",
    "\n",
    "| No. | Question                                                                                                   |  You Answer |\n",
    "|---------------------------------|--------------| ------- |\n",
    "| (1)  | `girvan_newman()` removes the edge with the largest Edge Betweenness Centrality value. For the 4 edges that get removed from the MRT Graph, why do the values increase after each iteration? |     |\n",
    "| (2)  | `girvan_newman()` always removes only one edge in each iteration even if 2 or more dges have the same maximum Edge Betweenness Centrality. Why is this meaningful, or why don't we remove all edge with the same maximum Edge Centrality in the same iteration? |      |\n",
    "| (3)  | `girvan_newman()` resolves ties by randomly picking one of the edges with the maximum Edge Betweenness Centrality. Are there better/smarter ways to pick which edge to choose |      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4a8d11",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}